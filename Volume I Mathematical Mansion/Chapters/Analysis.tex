\chapter{Mathematical Analysis}
\thispagestyle{fancy}

\begin{defn}[Field\index{Field}]{1}
	A \textit{field} $F$ can be defined as a commutative ring with identity such that for each $a\in F$ where $a \neq 0$, there is an element $a^{-1} \in F$ such that $a a^{-1}=1$.
\end{defn}

\begin{theo}[Rational Zeros Theorem\index{Rational Zeros Theorem}\cite{Elementary Analysis}]{1}
	Suppose $c_0, c_1, \dots,c_n$ are integers and r is a rational number satisfying the polynomial equation
	\begin{align}
		c_nx^n+c_{n-1}x^{n-1}+\cdots + c_1x+c_0=0 \label{RZT}
	\end{align}
	where $n\geq 1, c_n \neq 0$ and $c_0 \neq 0$. Let $r=\frac{c}{d}$ where $c,d$ are integers having no common factors and $d \neq 0$. Then $c$ divides $c_0$ and $d$ divides $c_n$.
	
	\hspace{1cm} In other words, the only rational candidates for solutions to (\ref{RZT}) have the form $\frac{c}{d}$ where $c$ divides $c_0$ and $d$ divides $c_n$.
\end{theo}

\begin{theo}[Properties of Absolute Values in $\mathbb{R}$]{1}
	For all $a,b \in \mathbb{R}$, $|a| \geq 0$, $|ab| = |a|\cdot|b|$, and $|a+b| \leq |a|+|b|$ (The \textit{triangle inequality}\index{Triangle inequality}).
\end{theo}

\begin{defn}[Minimum\index{Minimum} and Maximum\index{Maximum} \cite{Elementary Analysis}]{1}
	Let $S$ be a non-empty subset of $\mathbb{R}$.
	\begin{enumerate}
		\item[(a)] If $S$ contains a largest element $s_m$ [that is $s_m \in S$ and $s \leq s_m$ for all $s \in S$], then we call $s_m$ the \textit{maximum} of $S$ and write $s_m = \max S$.
		
		\item[(b)] If $S$ contains a smallest element $s_m$ [that is $s_m \in S$ and $s_m \leq s$ for all $s \in S$], then we call $s_m$ the \textit{minimum} of $S$ and write $s_m = \min S$.
	\end{enumerate}
\end{defn}

\begin{defn}[Bounded Sets \cite{Elementary Analysis}]{1}
	Let $S$ be a non-empty subset of $\mathbb{R}$.
	\begin{enumerate}
		\item[(a)] If $M \in \mathbb{R}$ satisfies $s \leq M$ for all $s \in S$, then $M$ is
		called an \textit{upper bound} of $S$ and the set $S$ is said to be bounded above.
		
		\item[(b)] If $m \in \mathbb{R}$ satisfies $m \leq s$ for all $s \in S$, then $m$ is
		called a \textit{lower bound} of $S$ and the set $S$ is said to be bounded below.
		
		\item[(c)] The set $S$ is said to be bounded if it is bounded above and bounded below. Thus $S$ is bounded if there exist $m,M \in \mathbb{R}$ such that $S \subseteq [m, M]$
	\end{enumerate}
\end{defn}

\newpage

\begin{defn}[Supremum\index{Supremum} and Infimum\index{Infimum} \cite{Elementary Analysis}]{1}
	Let $S$ be a non-empty subset of $\mathbb{R}$.
	\begin{enumerate}
		\item[(a)] If $S$ is bounded above and $S$ has a least upper bound, then we will call it the \textit{supremum} of $S$ and denote it $\sup S$.
		
		\item[(b)] If $S$ is bounded below and $S$ has a greatest lower bound, then we will call it the \textit{infimum} of $S$ and denote it $\inf S$.
	\end{enumerate}
\end{defn}

\begin{defn}[Completeness Axiom\index{Completeness Axiom} \cite{Elementary Analysis}]{1}
	Every nonempty subset $S$ of $\mathbb{R}$ that is bounded above has a least upper bound. In other words, $\sup S$ exists and is a real number.
\end{defn}

\begin{defn}[Archimedean Property\index{Archimedean property} \cite{Elementary Analysis}]{1}
	If $a>0$ and $b>0$, then for some positive integer $n$, we have $na >b$.
\end{defn}

\begin{defn}[Denseness of $\mathbb{Q}$\index{Denseness of $\mathbb{Q}$} \cite{Elementary Analysis}]{1}
	If $a,b \in \mathbb{R}$ and $a < b$, then there is a rational $r \in \mathbb{Q}$ such that $a < r < b$.
\end{defn}

\section{Sequences and Series}

\begin{defn}[A Sequence\index{Sequence}]{1}
	A sequence is a function $\{n\in\mathbb{Z}:n\geq m\} \rightarrow \mathbb{R},n\mapsto s_n$.
	\begin{enumerate}
		\item $m=0$ or $m=1$ most often.
		\item Often denoted as $(s_1,s_2,s_3,\dots)$, $(s_n)_{n=m}^\infty$, $(s_n)_{n\in\mathbb{N}}$, or $(s_n)$.		
	\end{enumerate}
\end{defn}

\begin{defn}[Limits\index{Limit}, Converging\index{Converge}, and Diverging\index{Diverge} \cite{Elementary Analysis}]{1}
	A sequence $(s_n)$ of real numbers is said to \textit{converge} to the real number $s$ provided that for each $\epsilon > 0$ there exists a number $N$	such that $n > N \implies |s_n-s|<\epsilon$. If $(s_n)$ converges to $s$, we will write $\lim_{n\rightarrow\infty} s_n = s$, or $s_n \rightarrow s$. The	number $s$ is called the \textit{limit} of the sequence $(s_n)$. A sequence that does not converge to some real number is said to \textit{diverge}.
\end{defn}

\newpage

\begin{theo}[Sequence and Limit Properties\index{Limit}\index{Sequence} \cite{Elementary Analysis}]{1}
	\begin{enumerate}[i]
		\item Convergent sequences are bounded.
		\item If the sequence $(s_n)$ converges to $s$ and $k\in\mathbb{R}$, then the sequence $(ks_n)$ converges to $ks$. That is, $\lim ks_n=k\cdot \lim s_n$.
		\item If the sequence $(s_n)$ converges to $s$ and $(t_n)$ converges to $t$, then $(s_n+t_n)$ converges to $s+t$. That is, $\lim s_n+t_n=\lim s_n+\lim t_n$.
		\item If the sequence $(s_n)$ converges to $s$ and $(t_n)$ converges to $t$, then $(s_n t_n)$ converges to $st$. That is, $\lim s_n\cdot t_n=(\lim s_n)(\lim t_n)$.
		\item Suppose $(s_n)$ converges to $s$ and $(t_n)$ converges to $t$. If $s \neq 0$ and
		$s_n \neq 0$ for all $n$, then $(\frac{t_n}{s_n})$ converges to $\frac{t}{s}$.		
		\item Let $(s_n)$ and $(t_n)$ be sequences such that $\lim s_n=\pm\infty$ and $\lim t_n > 0$. Then $\lim s_n t_n = \pm\infty$.
		\item For a sequence $(s_n)$ of positive real numbers, we have $\lim s_n = \infty$
		if and only if $\lim(\frac{1}{s_n}) = 0$.
		\item All bounded monotonic sequences converge.
		\item If $(s_n)$ is an unbounded increasing or decreasing sequence, then $\lim s_n = \pm\infty$ respectively.
		\item If $t \in \mathbb{R}$, then there is a subsequence of $(s_n)$ converging to $t$ if and only if the set $\{n \in \mathbb{N} : |s_n - t| < \epsilon\}$ is infinite for all $\epsilon > 0$.
		\item If the sequence $(s_n)$ is unbounded above or below, it has a subsequence with limit $\pm \infty$ respectively.
		\item Let $(s_n)$ be any sequence. There exists a monotonic subsequence	whose limit is $\lim \sup s_n$, and there exists a monotonic subsequence	whose limit is $\lim \inf s_n$.
	\end{enumerate}
\end{theo}


\begin{defn}[Divergence\index{Divergence} \cite{Elementary Analysis}]{1}
	Let $(s_n)$ be a sequence.
	\begin{enumerate}[i]
		\item We write $\lim s_n = \infty$ (diverges to $+\infty$) provided for each $M>0$, there is a number $N$ such that $n>N \implies s_n>M$.
		\item We write $\lim s_n = -\infty$ (diverges to $-\infty$) provided for each $M<0$, there is a number $N$ such that $n>N \implies s_n<M$.
	\end{enumerate}	
\end{defn}

\begin{defn}[Monotonic Sequences\index{Monotonic Sequences} \cite{Elementary Analysis}]{1}
	A sequence $(s_n)$ of real numbers is called an \textit{increasing sequence}	if $s_n \leq s_{n+1}$ for all $n$, and $(s_n)$ is called a \textit{decreasing sequence} if
	$s_n \geq s_{n+1}$ for all $n$. Note that if $(s_n)$ is increasing, then $s_n \leq s_m$
	whenever $n < m$. A sequence that is increasing or decreasing will	be called a \textit{monotone sequence} or a \textit{monotonic sequence}.
	\begin{enumerate}[(i)]
		\item Monotonic sequences will always either converge, or diverge to $\pm \infty$.
	\end{enumerate}
\end{defn}

\newpage

\begin{theo}[Limits of Supremum\index{Supremum} and Infimum\index{Infimum} \cite{Elementary Analysis}]{1}
	Let $(s_n)$ be a sequence in $\mathbb{R}$.
	\begin{enumerate}[(i	)]
		\item If $\lim s_n$ is defined (as a real number or $\pm\infty$), then $\lim \inf s_n = \lim s_n = \lim \sup s_n$.
		\item If $\lim \inf s_n = \lim \sup s_n$, then $\lim s_n$ is defined and $\lim s_n =
		\lim \inf s_n = \lim \sup s_n$.
		\item If $(s_n)$ converges to a positive real number $s$ and $(t_n)$ is any sequence, then $\lim \sup s_n t_n = s\cdot \lim \sup t_n$.
		\item If $(s_n)$ is any sequence of nonzero real numbers then
		\begin{align}
			\lim \inf \left|\frac{s_{n+1}}{s_n}\right| \leq \lim \inf |s_n|^{1/n} \leq \lim \sup |s_n|^{1/n} \leq \lim \sup \left|\frac{s_{n+1}}{s_n}\right| \nonumber
		\end{align}
	\end{enumerate}
\end{theo}

\begin{defn}[Cauchy Sequence\index{Cauchy Sequence} \cite{Elementary Analysis}]{1}
	A sequence $(s_n)$ of real numbers is called a \textit{Cauchy sequence} if for each $\epsilon > 0$ there exists a number $N$ such that $m,n > N \implies |s_n-s_m|<\epsilon$.
	\begin{enumerate}[(i)]
		\item Cauchy sequences are bounded.
		\item A sequence is a convergent sequence if and only if it is a Cauchy sequence. 
	\end{enumerate}	
\end{defn}

\begin{defn}[Subsequence\index{Subsequence} \cite{Elementary Analysis}]{1}
	Suppose $(s_n)_{n\in N}$ is a sequence. A \textit{subsequence} of this sequence is a sequence of the form $(t_k)_{k\in N}$ where for each $k$ there is a positive	integer $n_k$ such that	$n_1 < n_2 < \cdots < n_k < n_{k+1} < \cdots$ and $t_k = s_{n_k}$.
	\begin{enumerate}[(i)]
		\item If the sequence $(s_n)$ converges, then every subsequence converges to the same limit.
		\item Every sequence $(s_n)$ has a monotonic subsequence.
		\item Every bounded sequence has a convergent subsequence (Bolzano-Weierstrass Theorem \index{Bolzano-Weierstrass Theorem}).
	\end{enumerate}
\end{defn}


\begin{defn}[Subsequential limit\index{Subsequential limit} \cite{Elementary Analysis}]{1}
	Let $(s_n)$ be a sequence in $\mathbb{R}$. A \textit{subsequential limit} (occasionally denoted $S$) is any real number or symbol $\pm \infty$ that is the limit of some subsequence of $(s_n)$.
	\begin{enumerate}[(i)]
		\item $S$ is non-empty.
		\item $\sup S = \lim \sup s_n$ and $\inf S = \lim \inf s_n$.
		\item $\lim s_n$ exists if and only if $S$ has exactly one element, namely	$\lim s_n$.
		\item Suppose $(t_n)$ is a sequence in $S\cap R$ and that $t = \lim t_n$. Then $t$ belongs to $S$.
	\end{enumerate}
\end{defn}

\begin{defn}[Cauchy Critereon\index{Cauchy Critereon} \cite{Elementary Analysis}]{1}
	We say a series $\sum a_n$ satisfies the \textit{Cauchy Critereon} if its subsequence ($s_n$) of partial sums is a Cauchy sequence: for each $\epsilon > 0$ there exists a number $N$ such that $m,n >N$ implies $|s_n-s_m|<\epsilon$. 
	\begin{enumerate}
		\item A series converges if and only if it satisfies the Cauchy criterion.
	\end{enumerate}
\end{defn}

\section{Convergence/Divergence Tests}

\begin{fancybox}[Comparison Test\index{Comparison Test} \cite{Elementary Analysis}]{1}
	Let $\sum a_n$ be a series where $a_n \geq 0$ for all $n$.
	\begin{enumerate}[(i)]
		\item If $\sum a_n$ converges and $|b_n| \leq a_n$ for all $n$, then $\sum b_n$ converges.
		\item If $\sum a_n = \infty$ and $b_n \geq a_n$ for all $n$, then $\sum b_n = \infty$. 
	\end{enumerate}
\end{fancybox}

\begin{fancybox}[Ratio Test\index{Ratio Test} \cite{Elementary Analysis}]{1}
	Let $\sum a_n$ be a series of nonzero terms.
	\begin{enumerate}[(i)]
		\item $\sum a_n$ converges absolutely if $\lim \sup \left| \frac{a_{n+1}}{a_n}\right| <1$.
		\item $\sum a_n$ diverges if $\lim \inf \left| \frac{a_{n+1}}{a_n}\right| >1$.
		\item Otherwise, $\lim \inf \left| \frac{a_{n+1}}{a_n}\right| \leq 1 \leq \lim \sup \left| \frac{a_{n+1}}{a_n}\right|$ and the test gives no information.
	\end{enumerate}
\end{fancybox}

\begin{fancybox}[Root Test\index{Root Test} \cite{Elementary Analysis}]{1}
	Let $\sum a_n$ be a series and let $\alpha = \lim \sum |a_n|^{1/n}$.
	\begin{enumerate}[(i)]
		\item The series $\sum a_n$ converges absolutely if $\alpha <1$.
		\item The series $\sum a_n$ diverges if $\alpha >1$.
	\end{enumerate}
\end{fancybox}


\begin{fancybox}[Integral Test\index{Integral Test} \cite{Elementary Analysis}]{1}
	\begin{enumerate}[(i)]
		\item If $\lim\limits_{n \rightarrow \infty} \int_{0}^{n} f(x) dx =\infty$ then the series $\sum f(x)$ will diverge.
		\item If $\lim\limits_{n \rightarrow \infty} \int_{0}^{n} f(x) dx <\infty$ then the series $\sum f(x)$ will converge.
	\end{enumerate}
\end{fancybox}

\newpage

\begin{theo}[Miscellaneous Convergent Properties and Theorems \cite{Elementary Analysis}]{1}
	\begin{enumerate}[(i)]
		\item If a series $\sum a_n$ converges, then $\lim a_n = 0$. 
		\item $\sum \frac{1}{n^p}$ converges if and only if $p > 1$. 
	\end{enumerate}
\end{theo}



\begin{theo}[Alternating Series Theorem\index{Alternating Series} \cite{Elementary Analysis}]{1}
	If $a_1 \geq a_2 \geq \cdots \geq a_n \geq \cdots \geq 0$ and $\lim a_n =0$, then the alternating series $\sum(-1)^{n+1}a_n$ converges. Moreover, the partial sums $s_n = \sum_{k=1}^{n}(-1)^{k+1}a_k$ satisfy $|s-s_n|\leq a_n$ for all $n$.
\end{theo}

\section{Functions}

\begin{defn}[Continuous Function\index{Continuous function} \cite{Elementary Analysis}]{1}
	Let $f$ be a real-valued function whose domain is a subset of $\mathbb{R}$. The function $f$ is \textit{continuous} at $x_0$ in dom($f$) if, for every sequence ($x_n$)	in dom($f$) converging to $x_0$, we have $\lim_n f(x_n) = f(x_0)$. If $f$ is continuous at each point of a set $S \subseteq$ dom($f$), then $f$ is said to	be \textit{continuous} on $S$. The function $f$ is said to be \textit{continuous} if it is continuous on dom($f$).
\end{defn}

\begin{theo}[Continuous Function \cite{Elementary Analysis}]{1}
	Let $f$ be a real-valued function with dom($f$) $\subseteq \mathbb{R}$. Then
	$f$ is \textit{continuous} at $x_0$ in dom($f$) if and only if for each $\epsilon > 0$, there exists $\delta > 0$ such that $x\in$ dom($f$) and $|x-x_0|<\delta$ imply $|f(x)-f(x_0)|<\epsilon$.
\end{theo}

\begin{defn}[Continuous Functions\index{Continuous functions} \cite{Elementary Analysis}]{1}
	Let $f$ be a real-valued function whose domain is a subset of $\mathbb{R}$.
	\begin{enumerate}[(i)]
		\item $f$ is continuous at $x_0$ in dom($f$) if, for every sequence ($x_n$)
		in dom($f$) converging to $x_0$ , we have $\lim_n f (x_n) = f(x_0)$. If $f$ is
		continuous at each point of a set $S \subseteq $ dom($f$), then $f$ is said to
		be continuous on $S$. The function $f$ is said to be continuous if it is continuous on dom($f$).
		\item $f$ is continuous at $x_0$ in dom($f$) if and only if for each $\epsilon > 0$ there exists $\delta >0$ such that $x\in$ dom($f$) and $|x-x_0|<\delta$ imply $|f(x)-f(x_0)|<\epsilon$.
		\item If $f$ is continuous
		at $x_0$ in dom($f$), then $|f|$ and $kf , k \in\mathbb{R}$, are continuous at $x_0$.
	\end{enumerate}
\end{defn}

\begin{theo}[\cite{Elementary Analysis}]{1}
	If $f$ is continuous at $x_0$ and $g$ is continuous at $f(x_0)$, then $g \circ f$ is continuous at $x_0$.
\end{theo}

\newpage

\begin{theo}[\cite{Elementary Analysis}]{1}
	Let $f$ and $g$ be real-valued functions that are continuous at $x_0$ in $\mathbb{R}$. Then
	\begin{enumerate}[(i)]
		\item $f+g$ is continuous at $x_0$;
		\item $fg$ is continuous at $x_0$;
		\item $\frac{f}{g}$ is continuous at $x_0$ if $g(x_0) \neq 0$.
	\end{enumerate}
\end{theo}

\begin{theo}[\cite{Elementary Analysis}]{1}
	Let $f$ be a continuous real-valued function on a closed interval $[a, b]$.
	Then $f$ is a bounded function. Moreover, $f$ assumes its maximum and minimum values on $[a, b]$; that is, there exist $x_0, y_0 \in [a, b]$ such that $f(x_0) \leq f(x) \leq f(y_0)$ for all $x \in [a, b]$.
\end{theo}

\begin{theo}[Intermediate Value Theorem\index{Intermediate Value Theorem} \cite{Elementary Analysis}]{1}
	If $f$ is a continuous real-valued function on an interval $I$, then $f$ has
	the intermediate value property on $I$: Whenever $a, b \in I, a < b$ and $y$
	lies between $f(a)$ and $f(b)$ [i.e., $f(a) < y < f(b)$ or $f(b) < y < f(a)$],
	there exists at least one $x \in (a, b)$ such that $f(x) = y$.
	\begin{enumerate}[(i)]
		\item \textit{Corollary.} If $f$ is a continuous real-valued function on an interval $I$, then the set $f(I) = \{f(x) : x \in I\}$ is also an interval or a single point.
	\end{enumerate}
\end{theo}

\begin{theo}[\cite{Elementary Analysis}]{1}
	Let $f$ be a continuous strictly increasing function on some interval $I$. Then $f(I)$ is an interval $J$ by the Intermediate Value Theorem and $f^{-1}$ represents a function with domain $J$. The function $f^{-1}$ is a continuous strictly increasing function on $J$.
\end{theo}

\begin{theo}[\cite{Elementary Analysis}]{1}
	Let $g$ be a strictly increasing function on an interval $J$ such that $g(J)$ is an interval $I$. Then $g$ is continuous on $J$.
\end{theo}

\begin{theo}[\cite{Elementary Analysis}]{1}
	Let $f$ be a one-to-one continuous function on an interval $I$. Then
	$f$ is strictly increasing $[x_1 < x_2 \implies f(x_1) < f(x_2)]$ or strictly
	decreasing $[x_1 < x_2 \implies f(x_1) > f(x_2)]$.
\end{theo}

\newpage

\begin{defn}[Uniformly Continuous\index{Uniformly Continuous} \cite{Elementary Analysis}]{1}
	Let $f$ be a real-valued function defined on a set $S \subseteq \mathbb{R}$. Then $f$ is \textit{Uniformly Continuous} on $S$ if for each $\epsilon > 0$ there exists $\delta > 0$ such that $x, y \in S$ and $|x-y| < \delta$ imply $|f(x)- f(y)| < \epsilon$. We will say $f$ is uniformly continuous if $f$ is \textit{uniformly continuous} on dom($f$).
	\begin{enumerate}
		\item If $f$ is continuous on a closed interval $[a, b]$, then $f$ is uniformly continuous on $[a, b]$.
		\item If $f$ is uniformly continuous on a set $S$ and ($s_n$) is a Cauchy sequence in $S$, then ($f(s_n)$) is a Cauchy sequence.
		\item A real-valued function $f$ on $(a, b)$ is uniformly continuous on $(a, b)$ if and only if it can be extended to a continuous function $\bar{f}$ on $[a, b]$.
		\item Let f be a continuous function on an interval $I$ [$I$ may be bounded	or unbounded]. Let $I^\circ$ be the interval obtained by removing from $I$	any endpoints that happen to be in $I$. If $f$ is differentiable on $I^\circ$ and if $f'$ is bounded on $I^\circ$, then $f$ is uniformly continuous on $I$.
	\end{enumerate}
\end{defn}

\begin{defn}[Limits of Functions \cite{Elementary Analysis}]{1}
	\begin{enumerate}[(i)]
		\item Let $S\subseteq\mathbb{R}$. Let $a$ and $L$ be real numbers or $\pm\infty$, where $a$ is the limit of some sequence in $S$. We write $\lim_{x\rightarrow a} s f(x) = L$ if $f$ is a function defined on $S$ and for every sequence $(x_n)$ in $S$ with limit $a$, we have $\lim_{n\rightarrow \infty} f(x_n) = L$.
		\item For $a \in \mathbb{R}$ and a function $f$ we write $\lim_{x\rightarrow a} f(x) = L$ provided $\lim_{x\rightarrow a} s f(x) = L$ for some set $S = J \ \{a\}$ where $J$ is an open interval containing $a$. $\lim_{x\rightarrow a} f(x)$ is called the \textit{two-sided} limit of $f$ at $a$. Note $f$ need not be defined at $a$ and, even if $f$	is defined at $a$, the value $f (a)$ need not equal $\lim_{x\rightarrow a} f(x)$. In fact, $f (a) = \lim_{x\rightarrow a} f(x)$ if and only if $f$ is defined on an open interval containing $a$ and $f$ is continuous at $a$.
		\item For $a \in\mathbb{R}$ and a function $f$ we write $\lim_{x\rightarrow a} + f(x) = L$ provided $\lim_{x\rightarrow a} s f(x) = L$ for some open interval $S = (a, b)$. $\lim_{x\rightarrow a} + f(x)$ is the \textit{right-hand} limit of $f$ at $a$. Again $f$ need not be defined at $a$.
		\item For $a \in\mathbb{R}$ and a function $f$ we write $\lim_{x\rightarrow a} - f(x) = L$ provided $\lim_{x\rightarrow a} s f(x) = L$ for some open interval $S = (c, a)$. $\lim_{x\rightarrow a} - f(x)$ is the \textit{left-hand} limit of $f$ at a.
		\item For a function $f$ we write $\lim_{x\rightarrow \infty} f(x) = L$ provided $\lim_{x\rightarrow \infty} s f(x) = L$ for some interval $S = (c, \infty)$. Likewise,	we write $\lim_{x\rightarrow -\infty} f(x) = L$ provided $\lim_{x\rightarrow -\infty} s f(x) = L$ for some interval $S = (-\infty, b)$.
	\end{enumerate}
\end{defn}

\newpage

\begin{theo}[Limits of Functions \cite{Elementary Analysis}]{1}
	Let $f_1$ and $f_2$ be functions for which the limits $L_1 = \lim_{x\rightarrow a} s f_1(x)$	and $L_2 = \lim_{x\rightarrow a} s f_2(x)$ exist and are finite. Then
	\begin{enumerate}[(i)]
		\item $\lim_{x\rightarrow a} s (f_1 + f_2)(x)$ exists and equals $L_1 + L_2$
		\item $\lim_{x\rightarrow a} s (f_1 f_2 )(x)$ exists and equals $L_1 L_2$
		\item $\lim_{x\rightarrow a} s (f_1 /f_2)(x)$ exists and equals $L_1 /L_2$ provided $L_2 \neq 0$
		and $f_2(x) \neq 0$ for $x \in S$.
	\end{enumerate}
\end{theo}


\begin{theo}[Limits of Functions \cite{Elementary Analysis}]{1}
	Let $f$ be a function for which the limit $L = \lim_{x\rightarrow a} s f(x)$ exists and
	is finite. If $g$ is a function defined on $\{f(x) : x \in S\} \cup \{L\}$ that is
	continuous at $L$, then $\lim_{x\rightarrow a} s g \odot f(x)$ exists and equals $g(L)$.
\end{theo}

\begin{theo}[Limits of Functions \cite{Elementary Analysis}]{1}
	Let $f$ be a function defined on a subset $S$ of $\mathbb{R}$, let $a$ be a real number that is the limit of some sequence in $S$, and let $L$ be a real number, then $\lim_{x\rightarrow a} s f(x) = L$ if and only if	for each $\epsilon > 0$ there exists $\delta > 0$ such that $x \in S$ and $|x - a| < \delta$ imply $|f(x) - L| < \epsilon$.
	\begin{enumerate}
		\item Let $f$ be a function defined on $J \ \{a\}$ for some open interval $J$ containing $a$, and let $L$ be a real number. Then $\lim_{x\rightarrow a} f(x) = L$ if and only if for each $\epsilon > 0$ there exists $\delta > 0$ such that $0 < |x - a| < \delta \implies |f(x) - L| < \epsilon$.
		\item Let $f$ be a function defined on some interval $(a, b)$, and let $L$ be a	real number. Then $\lim_{x\rightarrow a^+}  f(x) = L$ if and only if	for each $\epsilon > 0$ there exists $\delta > 0$ such that $0 < x < a + \delta \implies |f(x) - L| < \epsilon$.
	\end{enumerate}
\end{theo}

\begin{theo}[\cite{Elementary Analysis}]{1}
	Let $f$ be a function defined on $J \ \{a\}$ for some open interval $J$ containing $a$. Then $\lim_{x\rightarrow a} f(x)$ exists if and only if the limits $\lim_{x\rightarrow a^+} f(x)$ and $\lim_{x\rightarrow a^-} f(x)$ both exist and are equal, in which case all three limits are equal.
\end{theo}

\section{Sequences and Series of Functions}

\begin{theo}[Convergence of Power Series \cite{Elementary Analysis}]{1}
	For a power series $\sum a_n x^n$, we say $\beta = \lim \sup |a_n|^{1/n}$ and the radius of convergence $R=\frac{1}{\beta}$. For $\beta = 0$, we set $R=\infty$ and for $\beta = \infty$, we set $R=0$. The power series will converge for $|x| < R$ and diverge for $|x| > R$.
\end{theo}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%5


\begin{defn}[Pointwise Convergence\index{Pointwise Convergence} \cite{Elementary Analysis}]{1}
	Let $(f_n)$ be a sequence of real-valued functions defined on a set $S \subseteq R$. The sequence $(f_n)$ converges pointwise [i.e., at each point] to a	function $f$ defined on $S$ if $\lim\limits_{n->\infty} f_n(x) = f(x)$	for all $x \in S$.
\end{defn}

\begin{defn}[\cite{Elementary Analysis}]{1}
	Let $(f_n)$ be a sequence of real-valued functions defined on a set
	$S \subseteq R$. The sequence $(f_n)$ converges uniformly on $S$ to a function $f$ defined on $S$ if for each $\epsilon > 0$ there exists a number $N$ such that $|f_n(x) − f(x)| < \epsilon$ for all $x \in S$ and all $n > N$. We write $\lim f_n = f$ uniformly on $S$ or $f_n \rightarrow f$ uniformly on $S$.
\end{defn}

\begin{theo}[Uniform Limits of Continuous Functions \cite{Elementary Analysis}]{1}
	The uniform limit of continuous functions is continuous. More precisely, let $(f_n)$ be a sequence of functions on a set $S \subseteq R$, suppose $f_n \rightarrow f$ uniformly on $S$, and suppose $S = \textrm{dom}(f)$. If each $f_n$ is
	continuous at $x_0 \in S$, then $f$ is continuous at $x_0$ . [So if each $f_n$ is
	continuous on $S$, then $f$ is continuous on $S$.]
\end{theo}


\begin{theo}[\cite{Elementary Analysis}]{1}
	Let $(f_n)$ be a sequence of continuous functions on $[a, b]$, and suppose $f_n \rightarrow f$ uniformly on $[a, b]$. Then $\lim\limits_{n\rightarrow \infty} \int_{a}^{b}f_n(x) dx = \int_{a}^{b} f(x) dx$
\end{theo}

\begin{defn}[Uniformly Cauchy \cite{Elementary Analysis}]{1}
	A sequence $(f_n)$ of functions defined on a set $S \subseteq R$ is uniformly Cauchy on $S$ if for each $\epsilon > 0$ there exists a number $N$ such that $|f_n(x) - f_m(x)| < \epsilon$ for all $x \in S$ and all $m, n > N$.
\end{defn}

\begin{theo}[\cite{Elementary Analysis}]{1}
	Let $(f_n)$ be a sequence of functions defined and uniformly Cauchy on a set $S \subseteq R$. Then there exists a function f on $S$ such that $f_n \rightarrow f$ uniformly on $S$.
\end{theo}

\begin{theo}[\cite{Elementary Analysis}]{1}
	Consider a series $\sum_{k=0}^{\infty}g_k$ of functions on a set $S \subseteq R$. Suppose each $g_k$ is continuous on $S$ and the series converges uniformly on $S$. Then the series $\sum_{k=0}^{\infty}g_k$ represents a continuous function on $S$.
\end{theo}

\begin{theo}[\cite{Elementary Analysis}]{1}
	If a series	$\sum_{k=0}^{\infty}g_k$ of functions satisfies the Cauchy criterion uniformly on a set $S$, then the series converges uniformly on $S$.
\end{theo}

\begin{defn}[Weierstrass M-test\index{Weierstrass M-test} \cite{Elementary Analysis}]{1}
	Let $(M_k)$ be a sequence of nonnegative real numbers where $\sum M_k < \infty$. If $|g_k(x)| \leq M_k$ for all $x$ in a set $S$, then $\sum g_k$ converges uniformly on $S$.
\end{defn}

\begin{defn}[Uniformly Converging Power Series \cite{Elementary Analysis}]{1}
	Let $\sum a_n x^n$ be a power series with radius of convergence $R > 0$
	[possibly $R = \infty$]. If $0 < R_1 < R$, then the power series converges uniformly on $[−R_1 , R_1]$ to a continuous function.
\end{defn}

\begin{theo}[\cite{Elementary Analysis}]{1}
	If the power series $\sum_{n=0}^{\infty}a_nx^n$ has radius of convergence $R$, then the power series $\sum_{n=0}^{\infty}na_nx^{n-1}$ and $\sum_{n=0}^{\infty}\frac{a_n}{n+1}x^{n+1}$	also have radius of convergence $R$.
\end{theo}

\begin{theo}[\cite{Elementary Analysis}]{1}
	Suppose $f (x) = \sum_{n=0}^{\infty} a_n x^n$ has radius of convergence $R > 0$. Then $\int_{0}^{x}f(t)dt = \sum_{n=0}^{\infty}\frac{a_n}{n+1}x^{n+1}$ for $|x|<R$
\end{theo}

\begin{theo}[\cite{Elementary Analysis}]{1}
	Let $f(x) = \sum_{n=0}^{\infty} a_n x^n$ have radius of convergence $R > 0$. Then $f$ is differentiable on $(−R, R)$ and $f'(x) = \sum_{n=0}^{\infty} na_nx^{n-1}$ for $|x|<R$.
\end{theo}

\begin{theo}[Abel’s Theorem \cite{Elementary Analysis}]{1}
	Let $f(x) = \sum_{n=0}^{\infty} a_n x^n$ be a power series with finite positive radius of convergence $R$. If the series converges at $x = R$, then $f$ is continuous at $x = R$. If the series converges at $x = −R$, then $f$ is continuous	at $x = −R$.
\end{theo}

\newpage

\section{Derivatives}

\begin{defn}[\cite{Elementary Analysis}]{1}
	Let $f$ be a real-valued function defined on an open interval containing a point $a$. We say $f$ is differentiable at $a$, or $f$ has a derivative at $a$, if the limit $\lim\limits_{x\rightarrow a} \frac{f(x)-f(a)}{x-a}$ exists and is finite. We will write $f'(a)$ for the derivative of $f$ at $a$: 
	\begin{align}
		f'(a) = \lim\limits_{x\rightarrow a} \frac{f(x)-f(a)}{x-a} = \lim\limits_{h\rightarrow 0} \frac{f(x+h)-f(x)}{h} = \lim\limits_{h\rightarrow 0} \frac{f(x+h)-f(x-h)}{2h},
	\end{align}
	whenever this limit exists and is finite.
\end{defn}

\begin{theo}[\cite{Elementary Analysis}]{1}
	If $f$ is differentiable at a point $a$, then $f$ is continuous at $a$.
\end{theo}

\begin{theo}[Differentiation formulas \cite{Elementary Analysis}]{1}
	Let $f$ and $g$ be functions that are differentiable at the point $a$. Each of the functions $cf$ (c a constant), $f +g$, $f g$ and $f /g$ is also differentiable at $a$, except $f /g$ if $g(a) = 0$ since $f /g$ is not defined at $a$ in this case. The formulas are
	\begin{enumerate}
		\item $(cf)'(a) = cf'(a)$;
		\item $(f + g)'(a) = f'(a) + g'(a)$;
		\item (product rule) $(fg)'(a) = f(a)g'(a) + f'(a)g(a)$;
		\item (quotient rule) $(f/g)'(a) = [g(a)f'(a) − f (a)g'(a)]/g^2(a)$ if $g(a) \neq 0$.
	\end{enumerate}
\end{theo}

\begin{theo}[Chain Rule\index{Chain Rule} \cite{Elementary Analysis}]{1}
	If $f$ is differentiable at $a$ and $g$ is differentiable at $f(a)$, then the composite function $g \circ f$ is differentiable at $a$ and we have $(g\circ f)'(a) = g'(f(a))f'(a)$.
\end{theo}

\begin{theo}[\cite{Elementary Analysis}]{1}
	If $f$ is defined on an open interval containing $x_0$, if $f$ assumes its	maximum or minimum at $x_0$, and if $f$ is differentiable at $x_0$ , then $f'(x_0) = 0$.
\end{theo}

\newpage

\begin{theo}[Rolle's Theorem \cite{Elementary Analysis}]{1}
	Let $f$ be a continuous function on $[a, b]$ that is differentiable on $(a, b)$ and satisfies $f (a) = f (b)$. There exists [at least one] $x \in (a, b)$ such that $f'(x) = 0$.
\end{theo}

\begin{theo}[Mean Value Theorem\index{Mean Value Theorem} \cite{Elementary Analysis}]{1}
	Let $f$ be a continuous function on $[a, b]$ that is differentiable on $(a, b)$. Then there exists [at least one] $x \in (a, b)$ such that $f'(a) = \frac{f(x)-f(a)}{x-a}$.
	\begin{enumerate}
		\item Let $f$ be a differentiable function on $(a, b)$ such that $f'(x) = 0$ for all	$x \in (a, b)$. Then $f$ is a constant function on $(a, b)$.
		\item Let $f$ and $g$ be differentiable functions on $(a, b)$ such that $f' = g'$ on $(a, b)$. Then there exists a constant $c$ such that $f (x) = g(x) + c$ for	all $x \in (a, b)$.
	\end{enumerate}
\end{theo}

\begin{defn}[Increasing and Decreasing Functions \cite{Elementary Analysis}]{1}
	Let f be a real-valued function defined on an interval I. 
	\begin{enumerate}[(i)]
		\item We say $f$ is \textit{strictly increasing} on $I$ if ($x_1,x_2 \in I$ and $x_1 < x_2) \implies f(x_1) < f(x_2)$.
		\item We say $f$ is \textit{strictly decreasing} on $I$ if ($x_1,x_2 \in I$ and $x_1 < x_2) \implies f(x_1) > f(x_2)$.
		\item We say $f$ is \textit{increasing} on $I$ if ($x_1,x_2 \in I$ and $x_1 < x_2) \implies f(x_1) \leq f(x_2)$.
		\item We say $f$ is \textit{decreasing} on $I$ if ($x_1,x_2 \in I$ and $x_1 < x_2) \implies f(x_1) \geq f(x_2)$.
	\end{enumerate}
	
\end{defn}

\begin{theo}[\cite{Elementary Analysis}]{1}
	Let $f$ be a differentiable function on an interval $(a, b)$. Then
	\begin{enumerate}[(i)]
		\item $f$ is \textit{strictly increasing} if $f'(x) > 0$ for all $x \in (a, b)$.
		\item $f$ is \textit{strictly decreasing} if $f'(x)< 0$ for all $x \in (a, b)$.
		\item $f$ is \textit{increasing} if $f'(x) \geq 0$ for all $x \in (a, b)$.
		\item $f$ is \textit{decreasing} if $f'(x) \leq 0$ for all $x \in (a, b)$.
	\end{enumerate}
\end{theo}

\begin{theo}[Intermediate Value Theorem\index{Intermediate Value Theorem} for Derivatives \cite{Elementary Analysis}]{1}
	Let $f$ be a differentiable function on $(a, b)$. If $a < x_1 < x_2 < b$, and	if $c$ lies between $f'(x_1)$ and $f'(x_2)$, there exists [at least one] $x$ in $(x_1 , x_2)$ such that $f'(x) = c$.
\end{theo}

\begin{theo}[\cite{Elementary Analysis}]{1}
	Let $f$ be a one-to-one continuous function on an open interval $I$, and	let $J = f (I)$. If $f$ is differentiable at $x_0 \in I$ and if $f'(x_0) \neq 0$, then $f^{−1}$ is differentiable at $y_0 = f(x_0)$ and	$(f^{-1})'(y_0) = \frac{1}{f'(x_0)}$.
\end{theo}

\begin{theo}[Generalized Mean Value Theorem\index{Mean Value Theorem} \cite{Elementary Analysis}]{1}
	Let $f$ and $g$ be continuous functions on $[a, b]$ that are differentiable on $(a, b)$. Then there exists [at least one] $x \in (a, b)$ such that $f'(x)[g(b) - g(a)] = g'(x)[f (b) - f (a)]$.
\end{theo}

\begin{defn}[L'Hospital's Rule \cite{Elementary Analysis}]{1}
	Let $s$ signify $a, a^+ , a^-$, or $\pm \infty$ where $a\in R$, and suppose $f$ and	$g$ are differentiable functions for which the limit $\lim\limits_{x\rightarrow s}\frac{f'(x)}{g'(x)}=L$ exists. If $\lim\limits_{x\rightarrow s} f(x) = \lim\limits_{x\rightarrow s} g(x)=0$ or $\lim\limits_{x\rightarrow s} |g(x)| = \infty$ then $\lim\limits_{x\rightarrow s}\frac{f(x)}{g(x)}=L$.
\end{defn}

\begin{defn}[Taylor Series \cite{Elementary Analysis}]{1}
	Let $f$ be a function defined on some open interval containing $c$. If $f$	possesses derivatives of all orders at $c$, then the series $\sum_{k=0}^{\infty} \frac{f^(k)(c)}{k!}(x-c)^k$ is called the \textit{Taylor series} for $f$ about $c$. For $n \geq 1$, remainder $R_n (x)$ is defined by $R_n(x) = f(x) - \sum_{k=0}^{n-1} \frac{f^(k)(c)}{k!}(x-c)^k$.
	is defined by
\end{defn}

\begin{theo}[Taylor's Theorem\index{Taylor's Theorem} \cite{Elementary Analysis}]{1}
	Let $f$ be defined on $(a, b)$ where $a < c < b$. Here we allow $a = -\infty$ or $b = \infty$. Suppose the nth derivative $f(n)$ exists on $(a, b)$. Then for each $x \neq c$ in $(a, b)$ there is some $y$ between $c$ and $x$ such that $R_n(x) = \frac{f^(n)(y)}{n!}(x-c)^n$.
\end{theo}

\begin{theo}[\cite{Elementary Analysis}]{1}
	Let $f$ be defined on $(a, b)$ where $a < c < b$, and suppose the nth
	derivative $f(n)$ exists and is continuous on $(a, b)$. Then for $x \in (a, b)$ we have $R_n(x) = \int_{c}^{x} \frac{(x-t)^{n-1}}{(n-1)!}f^{(n)}(t) dt$.
\end{theo}

\begin{theo}[Binomial Series Theorem\index{Binomial Series Theorem} \cite{Elementary Analysis}]{1}
	If $\alpha \in \mathbb{R}$ and $|x| < 1$, then $(1+x)^\alpha = 1+\sum_{k=1}^{\infty} \frac{\alpha(\alpha-1)\cdots(\alpha-k+1)}{k!}x^k$.
\end{theo}
	

